# HealthTech ETL Pipeline

## Overview
This project is a local ETL (Extract, Transform, Load) pipeline designed to process doctor and appointment data from Excel files and load it into a PostgreSQL database. It is built using Python and Docker.

## Project Structure
```
ETLDataEngineer/
├── data/
│   └── raw/                # Raw input files (doctors.xlsx, appointments.xlsx)
├── logs/                   # Execution logs
├── notebooks/              # Jupyter notebooks for EDA
├── sql/                    # SQL queries for analysis
├── src/
│   ├── db.py               # Database connection and schema init
│   ├── etl.py              # Main ETL pipeline script
│   └── create_notebook.py  # Helper to generate EDA notebook
├── docker-compose.yml      # PostgreSQL Docker configuration
├── requirements.txt        # Python dependencies
└── README.md               # This file
```

## Setup Instructions

### Prerequisites
- Python 3.8+
- Docker & Docker Compose

### Step 1: Initialize Environment
1.  Create a virtual environment:
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
2.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

### Step 2: Start Database
Start the PostgreSQL container:
```bash
sudo docker-compose up -d
```
*Note: Ensure port 5432 is free.*

### Step 3: Run ETL Pipeline
Execute the pipeline script:
```bash
python src/etl.py
```
Check `logs/etl_pipeline.log` for execution details.

## Pipeline Explanation

### 1. Extract
- Reads `.xlsx` files using `pandas`.
- Logs the number of records extracted.

### 2. Transform
- **Cleaning**: Standardizes column names to lowercase.
- **Validation**:
    - Drops appointments with missing `patient_id`.
    - Converts `booking_date` to datetime objects.
- **Standardization**:
    - Normalizes `status` values to lowercase (e.g., 'Predicted' -> 'predicted', 'canceled' -> 'cancelled').
    - Ensures correct data types for IDs.

### 3. Load
- **Target**: Local PostgreSQL database (`healthtech` schema).
- **Strategy**: Idempotent load using `TRUNCATE` + `INSERT`.
    - Tables are truncated with `CASCADE` to handle foreign keys.
    - Data is bulk inserted using `to_sql`.

## AWS Architecture Proposal
In a production environment on AWS, I would propose the following architecture:

| Stage | AWS Tool | Justification |
| :--- | :--- | :--- |
| **Extract** | **S3 + Lambda** | Upload raw Excel files to an S3 bucket. An S3 Event Trigger invokes a Lambda function to start processing. S3 provides durable logic storage, and Lambda is cost-effective for event-driven extraction. |
| **Transform** | **AWS Glue** | For scalable, serverless data integration. Glue jobs (Python/Spark) can handle complex transformations, schema evolution, and large datasets efficiently. For smaller datasets, Lambda with Pandas layers might suffice, but Glue is more robust for scaling. |
| **Load** | **Amazon RDS (PostgreSQL)** | Managed relational database service. Handles backups, patching, and scaling. Alternatively, **Amazon Redshift** if the goal is purely analytical warehousing with massive data volume. |
| **Orchestration** | **AWS Step Functions** | To coordinate the workflow (S3 -> Glue -> RDS) and handle retries/failures gracefully. |

---
*Generated by Antigravity*
